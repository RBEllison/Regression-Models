ggplot(grad.school, aes(x = Year)) +
geom_line(aes(y = CumSumGrad, color= 1), lwd = 3, show.legend = FALSE) +
geom_line(aes(y = CumSumNoGrad, color= 2), lwd = 3,show.legend = FALSE) +
theme_bw() +
ggtitle('Cummuluative sums of grad vs no grad school') +
geom_vline(xintercept = 12.2, color = "red", size=1.5)
ggplot(grad.school, aes(x = Year)) +
geom_line(aes(y = CumSumGrad, color= 1), lwd = 1, show.legend = FALSE) +
geom_line(aes(y = CumSumNoGrad, color= 2), lwd = 1,show.legend = FALSE) +
theme_bw() +
ggtitle('Cummuluative sums of grad vs no grad school') +
geom_vline(xintercept = 12.2, color = "red", size=1.5)
ggplot(grad.school, aes(x = Year)) +
geom_line(aes(y = CumSumGrad, color= 1), lwd = 1, show.legend = FALSE) +
geom_line(aes(y = CumSumNoGrad, color= 2), lwd = 1,show.legend = FALSE) +
theme_bw() +
ggtitle('Cummuluative sums of grad vs no grad school') +
geom_vline(xintercept = 12, color = "red", size=1.5)
knitr::opts_chunk$set(echo = TRUE)
startups <- read.csv('../data/50_Startups.csv')
View(startups)
show(startups)
startups <- read.csv('../data/50_Startups.csv')
startups
View(startups)
knitr::opts_chunk$set(echo = TRUE)
library(caTools)
library(ggplot2)
summary(regressor)
# Fitting simple linear regression to the training set
regressor <- lm(formula = Salary ~ YearsExperience,
data = training.set)
# Set seed
set.seed(1)
# Importing salary data
salary <- read.csv('../data/Salary_Data.csv')
# Splitting the dataset into the training set and test set
split <- sample.split(salary$Salary, SplitRatio = 2/3)
training.set <- subset(salary, split == TRUE)
test.set <- subset(salary, split == FALSE)
# Fitting simple linear regression to the training set
regressor <- lm(formula = Salary ~ YearsExperience,
data = training.set)
# Predicting the test set results
y.pred <- predict(regressor, newdata = test.set)
# Visualizing the training set results
ggplot() +
geom_point(aes(x = training.set$YearsExperience, y = training.set$Salary), col = 'lightblue', lw = 4) +
geom_line(aes(x = training.set$YearsExperience, y = predict(regressor, newdata = training.set)), col = 'darkblue') +
theme_bw() +
ggtitle('Salary VS Experience (Training Set)') +
xlab('Years of Experience') +
ylab('Salary')
# Visualizing the test set results
ggplot() +
geom_point(aes(x = test.set$YearsExperience, y = test.set$Salary), col = 'lightblue', lw = 4) +
geom_line(aes(x = training.set$YearsExperience, y = predict(regressor, newdata = training.set)), col = 'darkblue') +
theme_bw() +
ggtitle('Salary VS Experience (Test Set)') +
xlab('Years of Experience') +
ylab('Salary')
summary(regressor)
knitr::opts_chunk$set(echo = TRUE)
library(catools)
library(caTools)
# Importing the dataset
startups <- read.csv('../data/50_Startups.csv')
# Splitting the data into test and training sets
split <- sample.split(startups$Profit, SplitRatio = .8)
training.set <- subset(startups, split == TRUE)
test.set <- subset(startups, split == FALSE)
startups$State <- factor(startups$State,
levels <- c('New York', 'California', 'Florida'),
labels <- c(1, 2, 3))
# Importing the dataset
startups <- read.csv('../data/50_Startups.csv')
# Dealing with categorical data - 'State'
# Encoding
startups$State <- factor(startups$State,
levels <- c('New York', 'California', 'Florida'),
labels <- c(1, 2, 3))
# Splitting the data into test and training sets
split <- sample.split(startups$Profit, SplitRatio = .8)
training.set <- subset(startups, split == TRUE)
test.set <- subset(startups, split == FALSE)
startups <- read.csv('../data/50_Startups.csv')
# Dealing with categorical data - 'State'
# Encoding
startups$State <- factor(startups$State,
levels <- c('New York', 'California', 'Florida'),
labels <- c(1, 2, 3))
# Splitting the data into test and training sets
set.seed(1)
split <- sample.split(startups$Profit, SplitRatio = .8)
training.set <- subset(startups, split == TRUE)
test.set <- subset(startups, split == FALSE)
regressor <-  lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, State , training.set)
regressor <-  lm(Profit ~ R.D.Spend + Administration + Marketing.Spend, State, training.set)
regressor <-  lm(Profit ~ R.D.Spend + Administration + Marketing.Spend + State, training.set)
regressor <-  lm(Profit ~ ., training.set)
summary(regressor)
# Regressor with only the R.D.Spend dependent variable
regressor <-  lm(Profit ~ R.D.Spend, training.set)
summary(regressor)
y.pred <- predict(regressor, newdata = test.set)
ggplot() +
geom_point(aes(x = test.set$R.D.Spend, y = test.set$Profit), col = 'lightblue', lw = 4) +
geom_line(aes(x = training.set$R.D.Spend, y = predict(regressor, newdata = training.set)), col = 'darkblue') +
theme_bw() +
ggtitle('Profit VS R.D.Spend (Test Set)') +
xlab('Research and Development Spend') +
ylab('Profit')
setwd("~/Desktop/Regressions/Regressions/Linear/Multiple")
knitr::opts_chunk$set(echo = TRUE)
# Importing the data
data <- read.csv('../../data/Position_Salaries.csv')
View(data)
# Importing the data
positions <- read.csv('../../data/Position_Salaries.csv')
split <- sample.split(positions$Salary, SplitRatio = .8)
training.set <- subset(positions, split == TRUE)
test.set <- subset(positions, split == FALSE)
library(caTools)
library(ggplot2)
# Set the seed
set.seed(1)
# Importing the data
positions <- read.csv('../../data/Position_Salaries.csv')
positions
# Saving the dataset with only the two necessary columns
positions <- positions[, 2:3]
View(positions)
dim(positions)
simple.regressor <- lm(Salary ~ Level, positions)
simple.regressor <- lm(formula = Salary ~ Level,
data = positions)
summary(simple.regressor)
positions$Level2 <- positions$Level ^ 2
# Create a new column which contains the squares of the levels
positions$Level2 <- positions$Level ^ 2
# Polynomial Regressor
polynomial.regressor <- lm(formula = Salary ~ .,
data = positions)
summary(polynomial.regressor)
positions$Level2 <- positions$Level ^ 2
positions$Level3 <- positions$Level ^ 3
# Polynomial Regressor
polynomial.regressor <- lm(formula = Salary ~ .,
data = positions)
summary(polynomial.regressor)
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(data = positions, aes(x = Level3, y = Salary), col = 'darkblue')
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(simple.regressor, newdata = positions)), col = 'darkblue')
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(simple.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Truth or Bluff (Simple Linear Regression') +
xlab('Level') +
ylab('Salary')
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(simple.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Truth or Bluff (Simple Linear Regression)') +
xlab('Level') +
ylab('Salary')
ggplot() +
geom_point(data = positions, aes(x = Level3, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level3, y = predict(polynomial.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Truth or Bluff (Ploynomial Linear Regression)') +
xlab('Level') +
ylab('Salary')
par(mfrow = 2)
par(mfrow = c(1,2))
par(mfrow = c(1,2))
# Visualizing the Simple Linear Regression
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(simple.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Truth or Bluff (Simple Linear Regression)') +
xlab('Level') +
ylab('Salary')
# Visualizing the Polynomial Linear Regression
ggplot() +
geom_point(data = positions, aes(x = Level3, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level3, y = predict(polynomial.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Truth or Bluff (Ploynomial Linear Regression)') +
xlab('Level') +
ylab('Salary')
# Visualizing the Simple Linear Regression
simple.plot <- ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(simple.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Truth or Bluff (Simple Linear Regression)') +
xlab('Level') +
ylab('Salary')
# Visualizing the Polynomial Linear Regression
poly.plot <- ggplot() +
geom_point(data = positions, aes(x = Level3, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level3, y = predict(polynomial.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Truth or Bluff (Ploynomial Linear Regression)') +
xlab('Level') +
ylab('Salary')
grid.arrange(simple.plot, poly.plot, ncol=2)
plot_grid(simple.plot, poly.plot, ncol=2)
library(cowplot)
knitr::opts_chunk$set(echo = TRUE)
library(caTools)
library(ggplot2)
simple.plot + poly.plot
library(gridExtra)
grid.arrange(simple.plot, poly.plot, ncol = 2)
knitr::opts_chunk$set(echo = TRUE)
library(caTools)
library(ggplot2)
library(gridExtra)
grid.arrange(simple.plot, poly.plot, ncol = 2)
simple.plot <- ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(simple.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Simple') +
xlab('Level') +
ylab('Salary')
# Visualizing the Polynomial Linear Regression
poly.plot <- ggplot() +
geom_point(data = positions, aes(x = Level3, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level3, y = predict(polynomial.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Polynomial') +
xlab('Level') +
ylab('Salary')
# Visualizing the Polynomial Linear Regression
poly.plot <- ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(polynomial.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Polynomial') +
xlab('Level') +
ylab('Salary')
grid.arrange(simple.plot, poly.plot, ncol = 2)
positions$Level2 <- positions$Level ^ 2
positions$Level3 <- positions$Level ^ 3
positions$Level4 <- positions$Level ^ 4
positions$Level5 <- positions$Level ^ 5
# Polynomial Regressor
polynomial.regressor <- lm(formula = Salary ~ .,
data = positions)
summary(polynomial.regressor)
poly.plot <- ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(polynomial.regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Polynomial') +
xlab('Level') +
ylab('Salary')
grid.arrange(simple.plot, poly.plot, ncol = 2)
y.pred <- predict(polynomial.regressor, newdata = 160000)
y.pred <- predict(polynomial.regressor, newdata = 6)
y.pred <- predict(polynomial.regressor, data.frame(Level = 6.5))
y.pred <- predict(polynomial.regressor, data.frame(Level2 = 6.5))
y.pred <- predict(simple.regressor, data.frame(Level = 6.5))
simple.y.pred <- predict(simple.regressor, data.frame(Level = 6.5))
simple.y.pred <- predict(simple.regressor, data.frame(Level = 6.5))
poly.y.pred <- predict(polynomial.regressor, data.frame(Level = 6.5,
Level2 = 6.5^2,
Level3 = 6.5^3,
Level4 = 6.5^4,
Level5 = 6.5^5))
simple.y.pred
poly.y.pred
simple.y.pred
poly.y.pred
# Create a new column which contains the squares of the levels
positions$Level2 <- positions$Level ^ 2
positions$Level3 <- positions$Level ^ 3
positions$Level4 <- positions$Level ^ 4
# Polynomial Regressor
polynomial.regressor <- lm(formula = Salary ~ .,
data = positions)
summary(polynomial.regressor)
simple.y.pred <- predict(simple.regressor, data.frame(Level = 6.5))
poly.y.pred <- predict(polynomial.regressor, data.frame(Level = 6.5,
Level2 = 6.5^2,
Level3 = 6.5^3,
Level4 = 6.5^4))
# Polynomial Regressor
polynomial.regressor <- lm(formula = Salary ~ .,
data = positions)
poly.y.pred <- predict(polynomial.regressor, data.frame(Level = 6.5,
Level2 = 6.5^2,
Level3 = 6.5^3,
Level4 = 6.5^4))
library(e1071)
svm?
4w
```{r}
# Set the seed
set.seed(1)
# Importing the data
positions <- read.csv('../../data/Position_Salaries.csv')
dim(positions)
positions
```
## Preparing the data
From the table we can see there is some redundancy between the $Position$ and $Level$ column. Therefor it would make sense to drop the $Position$ column and just use the numeric $Level$ and $Salary$ columns. Since we only have 10 observations, it would not be useful to split the data into a training and test set.
```{r}
# Saving the dataset with only the two necessary columns
positions <- positions[, 2:3]
```
## Simple Regression
```{r}
regressor <- svm(formula = Salary ~ Level,
data = positions,
type = 'eps-regression')
summary(regressor)
```
# Set the seed
set.seed(1)
# Importing the data
positions <- read.csv('../../data/Position_Salaries.csv')
dim(positions)
positions
# Saving the dataset with only the two necessary columns
positions <- positions[, 2:3]
regressor <- svm(formula = Salary ~ Level,
data = positions,
type = 'eps-regression')
summary(regressor)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
poly.plot <- ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(regressor, newdata = positions)), col = 'darkblue') +
ggtitle('SVR') +
xlab('Level') +
ylab('Salary')
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(regressor, newdata = positions)), col = 'darkblue') +
ggtitle('SVR') +
xlab('Level') +
ylab('Salary')
library(rpart)
# Set the seed
set.seed(1)
# Importing the data
positions <- read.csv('../../data/Position_Salaries.csv')
# Examine the Data
dim(positions)
positions
# Saving the dataset with only the two necessary columns
positions <- positions[, 2:3]
regressor <- rpart(formula = Salary ~ Level,
data = positions)
y.pred <- predict(regressor, data.frame(Level = 6.5))
# Visualizing the Polynomial Linear Regression
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Decision Tree Model') +
xlab('Level') +
ylab('Salary')
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Decision Tree Model') +
xlab('Level') +
ylab('Salary')
regressor <- rpart(formula = Salary ~ Level,
data = positions)
regressor <- rpart(formula = Salary ~ Level,
data = positions,
control = rpart.control(minsplit = 1))
y.pred <- predict(regressor, data.frame(Level = 6.5))
# Visualizing the Polynomial Linear Regression
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = positions$Level, y = predict(regressor, newdata = positions)), col = 'darkblue') +
ggtitle('Decision Tree Model') +
xlab('Level') +
ylab('Salary')
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
x_grid = seq(min(positions$Level), max(positions$Level), 0.1)
# Visualizing the Decision Tree Regression
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))), col = 'darkblue') +
ggtitle('Decision Tree Model') +
xlab('Level') +
ylab('Salary')
x_grid = seq(min(positions$Level), max(positions$Level), 0.01)
# Increase the resolution
x_grid = seq(min(positions$Level), max(positions$Level), 0.01)
# Visualizing the Decision Tree Regression
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))), col = 'darkblue') +
ggtitle('Decision Tree Model') +
xlab('Level') +
ylab('Salary')
knitr::opts_chunk$set(echo = TRUE)
library(caTools)
library(ggplot2)
library(gridExtra)
library(rpart)
# Set the seed
set.seed(1)
# Importing the data
positions <- read.csv('../../data/Position_Salaries.csv')
# Examine the Data
dim(positions)
positions
# Saving the dataset with only the two necessary columns
positions <- positions[, 2:3]
install.packages('randomForest')
#install.packages('randomForest')
library(randomForest)
#install.packages('randomForest')
library(randomForest)
regressor <- randomForest(x = positions[1], y = positions$Salary)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
regressor <- randomForest(x = positions[1],
y = positions$Salary,
trees = 10)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 10)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 100)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 1000)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
set.seed(1)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 1000)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
set.seed(1234)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 1000)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
set.seed(1234)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 10000)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
set.seed(1234)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 10000000)
set.seed(1234)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 10000)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
set.seed(1234)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 100000)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
set.seed(1234)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 1000000)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
set.seed(1234)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 10)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
# Increase the resolution
x_grid = seq(min(positions$Level), max(positions$Level), 0.01)
# Visualizing the random forest regression
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))), col = 'darkblue') +
ggtitle('Random Forest Regression Model') +
xlab('Level') +
ylab('Salary')
set.seed(1234)
regressor <- randomForest(x = positions[1],
y = positions$Salary,
ntree = 1000)
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
# Increase the resolution
x_grid = seq(min(positions$Level), max(positions$Level), 0.01)
# Visualizing the random forest regression
ggplot() +
geom_point(data = positions, aes(x = Level, y = Salary), col = 'lightblue') +
geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(Level = x_grid))), col = 'darkblue') +
ggtitle('Random Forest Regression Model') +
xlab('Level') +
ylab('Salary')
y.pred <- predict(regressor, data.frame(Level = 6.5))
y.pred
